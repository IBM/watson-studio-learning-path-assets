{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Customer Churn Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is using customer churn data from Kaggle (https://www.kaggle.com/sandipdatta/customer-churn-analysis) and has been adopted from the notebook available on Kaggle developed by SanD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook will go through the following steps:\n",
    "    1. Import Dataset\n",
    "    2. Analyze the Data\n",
    "    3. Prepare the data model building\n",
    "    4. Split data in test and train data\n",
    "    5. Train model using various machine learning algorithms for binary classification\n",
    "    6. Evaluate the models\n",
    "    7. Select the model best fit for the given data set\n",
    "    8. Save and deploy model to Watson Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "from sklearn import neighbors\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "from IPython.display import Image\n",
    "import matplotlib as mlp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset can be downloaded from https://www.kaggle.com/becksddf/churn-in-telecoms-dataset/data. Then upload it to IBM Watson Studio and insert the code to read the data using \"insert to code > Insert panndas DataFrame\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "import types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "client_abc8adf276ed4155b85320acd6c03a69 = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='TKUC6Q_jaZ3cpxrcu9u0JlrDIno7IWRERrFT_Npi3U1b',\n",
    "    ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n",
    "\n",
    "body = client_abc8adf276ed4155b85320acd6c03a69.get_object(Bucket='richhproject1-donotdelete-pr-vdi0oj1mgnvqrg',Key='customer-churn-kaggle.csv')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "df_data_1 = pd.read_csv(body)\n",
    "df_data_1.head()\n",
    "df = df_data_1\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the first 5 lines of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"churn\"].value_counts()\n",
    "sns.barplot(y.index, y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_True = df[\"churn\"][df[\"churn\"] == True]\n",
    "print (\"Churn Percentage = \"+str( (y_True.shape[0] / df[\"churn\"].shape[0]) * 100 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Analysis of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Churn by State "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"state\", \"churn\"]).size().unstack().plot(kind='bar', stacked=True, figsize=(30,10)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Churn by Area Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"area code\", \"churn\"]).size().unstack().plot(kind='bar', stacked=True, figsize=(5,5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Churn by customers with International Plan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"international plan\", \"churn\"]).size().unstack().plot(kind='bar', stacked=True, figsize=(5,5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Churn By Customers with Voice mail plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"voice mail plan\", \"churn\"]).size().unstack().plot(kind='bar', stacked=True, figsize=(5,5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following preprocessing steps need to be done:\n",
    "1. Turn categorical variables into discrete numerical variables\n",
    "2. Create response vector\n",
    "3. Drop superflous columns\n",
    "4. Build feature matrix\n",
    "5. Standardize feature matrix values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discreet value integer encoder\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# State, international plans and voice mail plan are strings and we want discreet integer values\n",
    "df['state'] = label_encoder.fit_transform(df['state'])\n",
    "df['international plan'] = label_encoder.fit_transform(df['international plan'])\n",
    "df['voice mail plan'] = label_encoder.fit_transform(df['voice mail plan'])\n",
    "\n",
    "print (df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create response vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['churn'].values.astype(np.int)\n",
    "y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop superflous columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop([\"Id\",\"Churn\"], axis = 1, inplace=True)\n",
    "df.drop([\"phone number\",\"churn\"], axis = 1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.values.astype(np.float)\n",
    "print(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Feature Matrix values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the data preparation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train/Test Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to adopt Stratified Cross Validation - Since the Response values are not balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_cv(X, y, clf_class, shuffle=True, n_folds=10):\n",
    "    stratified_k_fold = model_selection.StratifiedKFold(n_splits=n_folds, shuffle=shuffle)\n",
    "    y_pred = y.copy()\n",
    "    # ii -> train\n",
    "    # jj -> test indices\n",
    "    for ii, jj in stratified_k_fold.split(X, y): \n",
    "        X_train, X_test = X[ii], X[jj]\n",
    "        y_train = y[ii]\n",
    "        clf = clf_class\n",
    "        clf.fit(X_train,y_train)\n",
    "        y_pred[jj] = clf.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Models and Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build models using a variety of approaches to see how they compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create classifiers\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gradient_boost = GradientBoostingClassifier()\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc_model = SVC(gamma='auto')\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k_neighbors = KNeighborsClassifier()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_regression = LogisticRegression(solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradient Boosting Classifier:  {:.2f}'.format(metrics.accuracy_score(y, stratified_cv(X, y, gradient_boost))))\n",
    "print('Support vector machine(SVM):   {:.2f}'.format(metrics.accuracy_score(y, stratified_cv(X, y, svc_model))))\n",
    "print('Random Forest Classifier:      {:.2f}'.format(metrics.accuracy_score(y, stratified_cv(X, y, random_forest))))\n",
    "print('K Nearest Neighbor Classifier: {:.2f}'.format(metrics.accuracy_score(y, stratified_cv(X, y, k_neighbors))))\n",
    "print('Logistic Regression:           {:.2f}'.format(metrics.accuracy_score(y, stratified_cv(X, y, logistic_regression))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now generate confusion matrices for the various models to analyze the prediction in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_ens_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, gradient_boost))\n",
    "sns.heatmap(grad_ens_conf_matrix, annot=True,  fmt='');\n",
    "title = 'Gradient Boosting'\n",
    "plt.title(title);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_svc_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, svc_model))\n",
    "sns.heatmap(svm_svc_conf_matrix, annot=True,  fmt='');\n",
    "title = 'SVM'\n",
    "plt.title(title);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_conf_matrix = metrics.confusion_matrix(y, stratified_cv(X, y, random_forest))\n",
    "sns.heatmap(random_forest_conf_matrix, annot=True,  fmt='');\n",
    "title = 'Random Forest'\n",
    "plt.title(title);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Gradient Boosting Classifier:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, gradient_boost))))\n",
    "print('Support vector machine(SVM):\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, svc_model))))\n",
    "print('Random Forest Classifier:\\n {}\\n'.format(metrics.classification_report(y, stratified_cv(X, y, random_forest))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting seems to do comparatively better for this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = ensemble.GradientBoostingClassifier()\n",
    "gbc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get Feature Importance from the classifier\n",
    "feature_importance = gbc.feature_importances_\n",
    "print (gbc.feature_importances_)\n",
    "feat_importances = pd.Series(gbc.feature_importances_, index=df.columns)\n",
    "feat_importances = feat_importances.nlargest(19)\n",
    "feat_importances.plot(kind='barh' , figsize=(10,10)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Deploy model to Watson Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create client to access our WML service\n",
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "\n",
    "wml_credentials = {\n",
    "  \"apikey\": \"****\",\n",
    "  \"iam_apikey_description\": \"Auto-generated for key ****\",\n",
    "  \"iam_apikey_name\": \"wdp-writer\",\n",
    "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",\n",
    "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/bc1bd51c396536dc7d5f81d5a4e****::serviceid:ServiceId-855c9709-e7f0-4fcc-8c48-93540cde****\",\n",
    "  \"instance_id\": \"****\",\n",
    "  \"password\": \"****\",\n",
    "  \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "  \"username\": \"****\"\n",
    "}\n",
    "\n",
    "client = WatsonMachineLearningAPIClient(wml_credentials)\n",
    "print(client.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use this cell to do any cleanup of previously created models and deployments\n",
    "client.repository.list_models()\n",
    "client.deployments.list()\n",
    "\n",
    "# client.repository.delete('GUID of stored model')\n",
    "# client.deployments.delete('GUID of deployed model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the model in WML\n",
    "meta_props={client.repository.ModelMetaNames.NAME: \"Gradient Boosting model to predict customer churn\"}\n",
    "published_model = client.repository.store_model(model=gbc, meta_props={client.repository.ModelMetaNames.NAME: \"Gradient Boosting model to predict customer churn\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new list of models\n",
    "client.repository.list_models()\n",
    "\n",
    "# get UID of our just stored model\n",
    "model_uid = client.repository.get_model_uid(published_model)\n",
    "print(\"Model id: {}\".format(model_uid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create deployment\n",
    "created_deployment = client.deployments.create(model_uid, name=\"customer_churn_model_gb\")\n",
    "\n",
    "# new list of deployments\n",
    "client.deployments.list()\n",
    "\n",
    "# get UID of our new deployment\n",
    "deployment_uid = client.deployments.get_uid(created_deployment)\n",
    "print(\"Deployment id: {}\".format(deployment_uid))\n",
    "print(created_deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scoring end point\n",
    "scoring_endpoint = client.deployments.get_scoring_url(created_deployment)\n",
    "print(scoring_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use our WML client to score our model\n",
    "\n",
    "# add some test data\n",
    "scoring_payload = {'fields': ['state', 'account length', 'area code', 'international plan', 'voice mail plan',\n",
    "        'number vmail messages', 'total day minutes', 'total day calls', 'total day charge', 'total eve minutes', 'total eve calls',\n",
    "        'total eve charge', 'total night minutes', 'total night calls', 'total night charge', 'total intl minutes', 'total intl calls',\n",
    "        'total intl charge', 'customer service calls'], \n",
    "                    'values': [[2,162,415,0,0,0,70.7,108,12.02,157.5,87,13.39,154.8,82,6.97,9.1,3,2.46,4]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score the model\n",
    "predictions = client.deployments.score(scoring_endpoint, scoring_payload)\n",
    "print('prediction',json.dumps(predictions, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach and code fragments have been adopted from the nootebook on Kaggle by Sandip Datta (https://www.kaggle.com/sandipdatta). \n",
    "The full original notebook can be viewed here: https://www.kaggle.com/sandipdatta/customer-churn-analysis#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
